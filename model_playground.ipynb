{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c008692-2218-4a1a-8f65-6a9c91a8ae7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax \n",
    "from jax import numpy as jnp\n",
    "import chex\n",
    "import gymnax\n",
    "from gymnax.environments.environment import EnvState, Environment, EnvParams\n",
    "\n",
    "from typing import Tuple, Any, Callable\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d7da67-60ff-42d2-80a9-37d6cde9ee34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import (\n",
    "    DQNTrainingArgs, DQNTrainState,\n",
    "    DQN, DQNParameters, DQNAgent,\n",
    "    select_action, compute_loss, update_target,\n",
    "    initialize_agent_state,\n",
    "    compute_loss_double_dqn,\n",
    "    SimpleDQNAgent,\n",
    "    DoubleDQNAgent\n",
    ")\n",
    "from buffer import ReplayBuffer, ReplayBufferStorage, FIFOBuffer, ParallelFIFOBuffer\n",
    "from trainer import agent_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd1c0bd-f5cc-452d-8799-85453efc63ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds_dqn = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2724fb87-f9f9-45f7-8eae-0c97f5c6a426",
   "metadata": {},
   "outputs": [],
   "source": [
    "for SEED in tqdm(range(40, 51)):\n",
    "    args = DQNTrainingArgs()\n",
    "    rng = jax.random.key(SEED)\n",
    "    rng, agent_init_rng = jax.random.split(rng, 2)\n",
    "    # create the agent and its optimization state\n",
    "    agent_state = SimpleDQNAgent.initialize_agent_state(SimpleDQNAgent.dqn, agent_init_rng, args)\n",
    "    # create the environment\n",
    "    env, env_params = gymnax.make('CartPole-v1')\n",
    "    def env_reset(rng: chex.PRNGKey) -> Tuple[chex.Array, EnvState]:\n",
    "        return env.reset(rng, env_params)\n",
    "    def env_step(rng: chex.PRNGKey, env_state: EnvState, action: chex.Array) -> Tuple[chex.Array, EnvState]:\n",
    "        return env.step(rng, env_state, action, env_params)\n",
    "    state_shape = env.observation_space(env_params).shape\n",
    "    n_actions = env.action_space().n\n",
    "    # create replay buffer storage\n",
    "    buffer_state = FIFOBuffer.init_buffer(buffer_size=args.fifo_buffer_size, state_shape=state_shape)\n",
    "    # reset the environment to start working with it\n",
    "    rng, reset_rng = jax.random.split(rng, 2)\n",
    "    obs, env_state = env_reset(reset_rng)\n",
    "    environment_step = jnp.array(0, dtype=jnp.int32)\n",
    "\n",
    "    # now we define the main function to update the agent and compile it\n",
    "    agent_iter = partial(agent_iteration, args, SimpleDQNAgent, FIFOBuffer, env_reset, env_step)\n",
    "    # donate_argnums=(2,) tells jax to optimize all operations with replay buffer\n",
    "    # and do them in-place\n",
    "    # the cost of this to that we need to recompile it every time we reinitialize the agent\n",
    "    # (because some functions get recreated). we could avoid this at the cost of much \n",
    "    # more complicated implementation.\n",
    "    agent_iter = jax.jit(agent_iter, donate_argnums=(2,)).lower(\n",
    "        # states\n",
    "        rng, agent_state, buffer_state, env_state,\n",
    "        # inputs\n",
    "        obs, environment_step\n",
    "    ).compile()\n",
    "    losses = []\n",
    "    steps = []\n",
    "    returns = []\n",
    "    while environment_step < args.sample_budget:\n",
    "        rng, agent_state, buffer_state, env_state, obs, environment_step, dqn_losses, eval_returns = agent_iter(\n",
    "            # configuration and randomness\n",
    "            # args, SimpleDQNAgent, FIFOBuffer, env_step,\n",
    "            # states\n",
    "            rng, agent_state, buffer_state, env_state,\n",
    "            # inputs\n",
    "            obs, environment_step\n",
    "        )\n",
    "        # print(environment_step)\n",
    "        steps.append(environment_step)\n",
    "        returns.append(eval_returns)\n",
    "        losses.append(dqn_losses)\n",
    "        # return returns, losses\n",
    "    seeds_dqn[SEED] = (steps, returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afdbff8-5988-4069-8029-8c67401dbfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,3))\n",
    "plt.tight_layout()\n",
    "mean_return = np.stack([seeds_dqn[k][1] for k in seeds_dqn.keys()]).mean(0)\n",
    "std_return = np.stack([seeds_dqn[k][1] for k in seeds_dqn.keys()]).std(0)\n",
    "plt.plot(steps, mean_return, label='dqn')\n",
    "plt.fill_between(steps, mean_return - std_return, mean_return + std_return, alpha=0.3)\n",
    "plt.ylim(0, 500)\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.savefig('dqn.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba85436-6ce8-4510-af63-677e3159dbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds_ddqn = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0694ce94-c049-4888-aff4-9e2e8ad7d8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for SEED in tqdm(range(40, 51)):\n",
    "    args = DQNTrainingArgs()\n",
    "    rng = jax.random.key(SEED)\n",
    "    rng, agent_init_rng = jax.random.split(rng, 2)\n",
    "    # create the agent and its optimization state\n",
    "    agent_state = DoubleDQNAgent.initialize_agent_state(DoubleDQNAgent.dqn, agent_init_rng, args)\n",
    "    # create the environment\n",
    "    env, env_params = gymnax.make('CartPole-v1')\n",
    "    def env_reset(rng: chex.PRNGKey) -> Tuple[chex.Array, EnvState]:\n",
    "        return env.reset(rng, env_params)\n",
    "    def env_step(rng: chex.PRNGKey, env_state: EnvState, action: chex.Array) -> Tuple[chex.Array, EnvState]:\n",
    "        return env.step(rng, env_state, action, env_params)\n",
    "    state_shape = env.observation_space(env_params).shape\n",
    "    n_actions = env.action_space().n\n",
    "    # create replay buffer storage\n",
    "    buffer_state = FIFOBuffer.init_buffer(buffer_size=args.fifo_buffer_size, state_shape=state_shape)\n",
    "    # reset the environment to start working with it\n",
    "    rng, reset_rng = jax.random.split(rng, 2)\n",
    "    obs, env_state = env_reset(reset_rng)\n",
    "    environment_step = jnp.array(0, dtype=jnp.int32)\n",
    "\n",
    "    # now we define the main function to update the agent and compile it\n",
    "    agent_iter = partial(agent_iteration, args, DoubleDQNAgent, FIFOBuffer, env_reset, env_step)\n",
    "    # donate_argnums=(2,) tells jax to optimize all operations with replay buffer\n",
    "    # and do them in-place\n",
    "    # the cost of this to that we need to recompile it every time we reinitialize the agent\n",
    "    # (because some functions get recreated). we could avoid this at the cost of a bit \n",
    "    # more complicated implementation.\n",
    "    agent_iter = jax.jit(agent_iter, donate_argnums=(2,)).lower(\n",
    "        # states\n",
    "        rng, agent_state, buffer_state, env_state,\n",
    "        # inputs\n",
    "        obs, environment_step\n",
    "    ).compile()\n",
    "    losses = []\n",
    "    steps = []\n",
    "    returns = []\n",
    "    while environment_step < args.sample_budget:\n",
    "        rng, agent_state, buffer_state, env_state, obs, environment_step, dqn_losses, eval_returns = agent_iter(\n",
    "            # states\n",
    "            rng, agent_state, buffer_state, env_state,\n",
    "            # inputs\n",
    "            obs, environment_step\n",
    "        )\n",
    "        # print(environment_step)\n",
    "        steps.append(environment_step)\n",
    "        returns.append(eval_returns)\n",
    "        losses.append(dqn_losses)\n",
    "        # return returns, losses\n",
    "    seeds_ddqn[SEED] = (steps, returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23603560-5a12-40ff-8992-17f49325fcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,3))\n",
    "plt.tight_layout()\n",
    "mean_return = np.stack([seeds_dqn[k][1] for k in seeds_dqn.keys()]).mean(0)\n",
    "std_return = np.stack([seeds_dqn[k][1] for k in seeds_dqn.keys()]).std(0)\n",
    "plt.plot(steps, mean_return, label='dqn')\n",
    "plt.fill_between(steps, mean_return - std_return, mean_return + std_return, alpha=0.3)\n",
    "\n",
    "mean_return = np.stack([seeds_ddqn[k][1] for k in seeds_ddqn.keys()]).mean(0)\n",
    "std_return = np.stack([seeds_ddqn[k][1] for k in seeds_ddqn.keys()]).std(0)\n",
    "plt.plot(steps, mean_return, label='double-dqn')\n",
    "plt.fill_between(steps, mean_return - std_return, mean_return + std_return, alpha=0.3)\n",
    "\n",
    "plt.ylim(0, 500)\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.savefig('double-dqn.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
